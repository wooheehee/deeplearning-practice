{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPrTaVplTR+X1D2JA6GBWN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wooheehee/deeplearning-practice/blob/main/CH6_1_MNIST_%EC%88%AB%EC%9E%90%EB%B6%84%EB%A5%98%EA%B8%B0_Autoencoder%2BSoftmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rZmv4A_zI6J_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\n",
        "x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])\n",
        "x_train, x_test = x_train / 255., x_test / 255."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNkNUh3VOsRW",
        "outputId": "bb046cba-19ff-4126-e82b-11a092d104ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate_RMSProp = 0.02\n",
        "learning_rate_GradientDescent = 0.5\n",
        "num_epochs = 100         \n",
        "batch_size = 256\n",
        "display_step = 1        \n",
        "input_size = 784         \n",
        "hidden1_size = 128        \n",
        "hidden2_size = 64"
      ],
      "metadata": {
        "id": "dkKlSJ5QPNbM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.shuffle(60000).batch(batch_size)"
      ],
      "metadata": {
        "id": "5-X0W4ZYP8JN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_normal_intializer_with_stddev_1():\n",
        "  return tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None)"
      ],
      "metadata": {
        "id": "3p6K9srRQIYO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(AutoEncoder, self).__init__()\n",
        "    self.hidden_layer_1 = tf.keras.layers.Dense(hidden1_size,\n",
        "                                                activation='sigmoid',\n",
        "                                                kernel_initializer=random_normal_intializer_with_stddev_1(),\n",
        "                                                bias_initializer=random_normal_intializer_with_stddev_1())\n",
        "    self.hidden_layer_2 = tf.keras.layers.Dense(hidden2_size,\n",
        "                                                activation='sigmoid',\n",
        "                                                kernel_initializer=random_normal_intializer_with_stddev_1(),\n",
        "                                                bias_initializer=random_normal_intializer_with_stddev_1())\n",
        "    self.hidden_layer_3 = tf.keras.layers.Dense(hidden1_size,\n",
        "                                                activation='sigmoid',\n",
        "                                                kernel_initializer=random_normal_intializer_with_stddev_1(),\n",
        "                                                bias_initializer=random_normal_intializer_with_stddev_1())\n",
        "    self.output_layer = tf.keras.layers.Dense(input_size,\n",
        "                                                activation='sigmoid',\n",
        "                                                kernel_initializer=random_normal_intializer_with_stddev_1(),\n",
        "                                                bias_initializer=random_normal_intializer_with_stddev_1())\n",
        "    \n",
        "  def call(self, x):\n",
        "    H1_output = self.hidden_layer_1(x)\n",
        "    H2_output = self.hidden_layer_2(H1_output)\n",
        "    H3_output = self.hidden_layer_3(H2_output)\n",
        "    X_reconstructed = self.output_layer(H3_output)\n",
        "\n",
        "    return X_reconstructed, H2_output"
      ],
      "metadata": {
        "id": "zupD_c1CTxCY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxClassifier(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(SoftmaxClassifier, self).__init__()\n",
        "    self.softmax_layer = tf.keras.layers.Dense(10,\n",
        "                                               activation='softmax',\n",
        "                                               kernel_initializer='zeros',\n",
        "                                               bias_initializer='zeros')\n",
        "  \n",
        "  def call(self, x):\n",
        "    y_pred = self.softmax_layer(x)\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "uWVxmS9EVCwF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def pretraining_mse_loss(y_pred, y_true):\n",
        "  return tf.reduce_mean(tf.pow(y_true - y_pred, 2))"
      ],
      "metadata": {
        "id": "vj03m8eRVqtZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def finetuning_cross_entropy_loss(y_pred_softmax, y):\n",
        "  return tf.reduce_mean(-tf.reduce_sum(y*tf.math.log(y_pred_softmax), axis=[1]))"
      ],
      "metadata": {
        "id": "BwVinWTVV4C9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretraining_optimizer = tf.optimizers.RMSprop(learning_rate_RMSProp, epsilon=1e-10)"
      ],
      "metadata": {
        "id": "YPOgrGvDWNQI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def pretraining_train_step(autoencoder_model, x):\n",
        "  y_true = x\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred, _ = autoencoder_model(x)\n",
        "    pretraining_loss = pretraining_mse_loss(y_pred, y_true)\n",
        "  gradients = tape.gradient(pretraining_loss, autoencoder_model.trainable_variables)\n",
        "  pretraining_optimizer.apply_gradients(zip(gradients, autoencoder_model.trainable_variables))"
      ],
      "metadata": {
        "id": "EEaT_NndZS8M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_optimizer = tf.optimizers.SGD(learning_rate_GradientDescent)"
      ],
      "metadata": {
        "id": "ilsZTmBUZ8yZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function \n",
        "def finetuning_train_step(autoencoder_model, softmax_classifier_model, x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred, extracted_features = autoencoder_model(x)\n",
        "    y_pred_softmax = softmax_classifier_model(extracted_features)\n",
        "    finetuning_loss = finetuning_cross_entropy_loss(y_pred_softmax, y)\n",
        "  autoencoder_encoding_variables = autoencoder_model.hidden_layer_1.trainable_variables + autoencoder_model.hidden_layer_2.trainable_variables\n",
        "  gradients = tape.gradient(finetuning_loss, autoencoder_encoding_variables + softmax_classifier_model.trainable_variables)\n",
        "  finetuning_optimizer.apply_gradients(zip(gradients, autoencoder_encoding_variables + softmax_classifier_model.trainable_variables))"
      ],
      "metadata": {
        "id": "SvN7eMyuaLFV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def compute_accuracy(y_pred_softmax, y):\n",
        "  correct_prediction = tf.equal(tf.argmax(y_pred_softmax, 1), tf.argmax(y, 1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "rLck-Gxzb0lA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AutoEncoder_model = AutoEncoder()"
      ],
      "metadata": {
        "id": "dpM1XNvscPMn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SoftmaxClassifier_model = SoftmaxClassifier()"
      ],
      "metadata": {
        "id": "J8rcsRnhOuiE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  # 모든 배치들에 대해서 최적화를 수행합니다.\n",
        "  for batch_x, _ in train_data:\n",
        "    _, pretraining_loss_print = pretraining_train_step(AutoEncoder_model, batch_x), pretraining_mse_loss(AutoEncoder_model(batch_x)[0], batch_x)\n",
        "  # 지정된 epoch마다 학습결과를 출력합니다.\n",
        "  if epoch % display_step == 0:\n",
        "    print(\"반복(Epoch): %d, Pre-Training 손실 함수(pretraining_loss): %f\" % ((epoch + 1), pretraining_loss_print))\n",
        "print(\"Step 1 : MNIST 데이터 재구축을 위한 오토인코더 최적화 완료(Pre-Training)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3WUDDaBOzzj",
        "outputId": "9d7d0559-00f3-4ad4-d2be-25f41733509b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "반복(Epoch): 1, Pre-Training 손실 함수(pretraining_loss): 0.051933\n",
            "반복(Epoch): 2, Pre-Training 손실 함수(pretraining_loss): 0.044133\n",
            "반복(Epoch): 3, Pre-Training 손실 함수(pretraining_loss): 0.037276\n",
            "반복(Epoch): 4, Pre-Training 손실 함수(pretraining_loss): 0.036266\n",
            "반복(Epoch): 5, Pre-Training 손실 함수(pretraining_loss): 0.034023\n",
            "반복(Epoch): 6, Pre-Training 손실 함수(pretraining_loss): 0.037164\n",
            "반복(Epoch): 7, Pre-Training 손실 함수(pretraining_loss): 0.029869\n",
            "반복(Epoch): 8, Pre-Training 손실 함수(pretraining_loss): 0.030387\n",
            "반복(Epoch): 9, Pre-Training 손실 함수(pretraining_loss): 0.030131\n",
            "반복(Epoch): 10, Pre-Training 손실 함수(pretraining_loss): 0.027222\n",
            "반복(Epoch): 11, Pre-Training 손실 함수(pretraining_loss): 0.028287\n",
            "반복(Epoch): 12, Pre-Training 손실 함수(pretraining_loss): 0.029076\n",
            "반복(Epoch): 13, Pre-Training 손실 함수(pretraining_loss): 0.022610\n",
            "반복(Epoch): 14, Pre-Training 손실 함수(pretraining_loss): 0.024619\n",
            "반복(Epoch): 15, Pre-Training 손실 함수(pretraining_loss): 0.024744\n",
            "반복(Epoch): 16, Pre-Training 손실 함수(pretraining_loss): 0.026640\n",
            "반복(Epoch): 17, Pre-Training 손실 함수(pretraining_loss): 0.023533\n",
            "반복(Epoch): 18, Pre-Training 손실 함수(pretraining_loss): 0.027660\n",
            "반복(Epoch): 19, Pre-Training 손실 함수(pretraining_loss): 0.026921\n",
            "반복(Epoch): 20, Pre-Training 손실 함수(pretraining_loss): 0.026049\n",
            "반복(Epoch): 21, Pre-Training 손실 함수(pretraining_loss): 0.021559\n",
            "반복(Epoch): 22, Pre-Training 손실 함수(pretraining_loss): 0.023086\n",
            "반복(Epoch): 23, Pre-Training 손실 함수(pretraining_loss): 0.022406\n",
            "반복(Epoch): 24, Pre-Training 손실 함수(pretraining_loss): 0.021363\n",
            "반복(Epoch): 25, Pre-Training 손실 함수(pretraining_loss): 0.021698\n",
            "반복(Epoch): 26, Pre-Training 손실 함수(pretraining_loss): 0.023069\n",
            "반복(Epoch): 27, Pre-Training 손실 함수(pretraining_loss): 0.022255\n",
            "반복(Epoch): 28, Pre-Training 손실 함수(pretraining_loss): 0.021068\n",
            "반복(Epoch): 29, Pre-Training 손실 함수(pretraining_loss): 0.022307\n",
            "반복(Epoch): 30, Pre-Training 손실 함수(pretraining_loss): 0.019416\n",
            "반복(Epoch): 31, Pre-Training 손실 함수(pretraining_loss): 0.020422\n",
            "반복(Epoch): 32, Pre-Training 손실 함수(pretraining_loss): 0.021441\n",
            "반복(Epoch): 33, Pre-Training 손실 함수(pretraining_loss): 0.019226\n",
            "반복(Epoch): 34, Pre-Training 손실 함수(pretraining_loss): 0.020361\n",
            "반복(Epoch): 35, Pre-Training 손실 함수(pretraining_loss): 0.020034\n",
            "반복(Epoch): 36, Pre-Training 손실 함수(pretraining_loss): 0.019905\n",
            "반복(Epoch): 37, Pre-Training 손실 함수(pretraining_loss): 0.018431\n",
            "반복(Epoch): 38, Pre-Training 손실 함수(pretraining_loss): 0.021519\n",
            "반복(Epoch): 39, Pre-Training 손실 함수(pretraining_loss): 0.017130\n",
            "반복(Epoch): 40, Pre-Training 손실 함수(pretraining_loss): 0.018025\n",
            "반복(Epoch): 41, Pre-Training 손실 함수(pretraining_loss): 0.015473\n",
            "반복(Epoch): 42, Pre-Training 손실 함수(pretraining_loss): 0.017194\n",
            "반복(Epoch): 43, Pre-Training 손실 함수(pretraining_loss): 0.016364\n",
            "반복(Epoch): 44, Pre-Training 손실 함수(pretraining_loss): 0.016303\n",
            "반복(Epoch): 45, Pre-Training 손실 함수(pretraining_loss): 0.015720\n",
            "반복(Epoch): 46, Pre-Training 손실 함수(pretraining_loss): 0.016234\n",
            "반복(Epoch): 47, Pre-Training 손실 함수(pretraining_loss): 0.015719\n",
            "반복(Epoch): 48, Pre-Training 손실 함수(pretraining_loss): 0.016603\n",
            "반복(Epoch): 49, Pre-Training 손실 함수(pretraining_loss): 0.015094\n",
            "반복(Epoch): 50, Pre-Training 손실 함수(pretraining_loss): 0.016687\n",
            "반복(Epoch): 51, Pre-Training 손실 함수(pretraining_loss): 0.014944\n",
            "반복(Epoch): 52, Pre-Training 손실 함수(pretraining_loss): 0.014701\n",
            "반복(Epoch): 53, Pre-Training 손실 함수(pretraining_loss): 0.014460\n",
            "반복(Epoch): 54, Pre-Training 손실 함수(pretraining_loss): 0.013696\n",
            "반복(Epoch): 55, Pre-Training 손실 함수(pretraining_loss): 0.013983\n",
            "반복(Epoch): 56, Pre-Training 손실 함수(pretraining_loss): 0.012408\n",
            "반복(Epoch): 57, Pre-Training 손실 함수(pretraining_loss): 0.012949\n",
            "반복(Epoch): 58, Pre-Training 손실 함수(pretraining_loss): 0.013706\n",
            "반복(Epoch): 59, Pre-Training 손실 함수(pretraining_loss): 0.015039\n",
            "반복(Epoch): 60, Pre-Training 손실 함수(pretraining_loss): 0.014914\n",
            "반복(Epoch): 61, Pre-Training 손실 함수(pretraining_loss): 0.014008\n",
            "반복(Epoch): 62, Pre-Training 손실 함수(pretraining_loss): 0.013210\n",
            "반복(Epoch): 63, Pre-Training 손실 함수(pretraining_loss): 0.014934\n",
            "반복(Epoch): 64, Pre-Training 손실 함수(pretraining_loss): 0.013195\n",
            "반복(Epoch): 65, Pre-Training 손실 함수(pretraining_loss): 0.013483\n",
            "반복(Epoch): 66, Pre-Training 손실 함수(pretraining_loss): 0.012954\n",
            "반복(Epoch): 67, Pre-Training 손실 함수(pretraining_loss): 0.011670\n",
            "반복(Epoch): 68, Pre-Training 손실 함수(pretraining_loss): 0.014286\n",
            "반복(Epoch): 69, Pre-Training 손실 함수(pretraining_loss): 0.012405\n",
            "반복(Epoch): 70, Pre-Training 손실 함수(pretraining_loss): 0.014593\n",
            "반복(Epoch): 71, Pre-Training 손실 함수(pretraining_loss): 0.016637\n",
            "반복(Epoch): 72, Pre-Training 손실 함수(pretraining_loss): 0.015272\n",
            "반복(Epoch): 73, Pre-Training 손실 함수(pretraining_loss): 0.011039\n",
            "반복(Epoch): 74, Pre-Training 손실 함수(pretraining_loss): 0.012210\n",
            "반복(Epoch): 75, Pre-Training 손실 함수(pretraining_loss): 0.014041\n",
            "반복(Epoch): 76, Pre-Training 손실 함수(pretraining_loss): 0.013206\n",
            "반복(Epoch): 77, Pre-Training 손실 함수(pretraining_loss): 0.013858\n",
            "반복(Epoch): 78, Pre-Training 손실 함수(pretraining_loss): 0.012106\n",
            "반복(Epoch): 79, Pre-Training 손실 함수(pretraining_loss): 0.012584\n",
            "반복(Epoch): 80, Pre-Training 손실 함수(pretraining_loss): 0.011547\n",
            "반복(Epoch): 81, Pre-Training 손실 함수(pretraining_loss): 0.013622\n",
            "반복(Epoch): 82, Pre-Training 손실 함수(pretraining_loss): 0.013234\n",
            "반복(Epoch): 83, Pre-Training 손실 함수(pretraining_loss): 0.012968\n",
            "반복(Epoch): 84, Pre-Training 손실 함수(pretraining_loss): 0.011340\n",
            "반복(Epoch): 85, Pre-Training 손실 함수(pretraining_loss): 0.011090\n",
            "반복(Epoch): 86, Pre-Training 손실 함수(pretraining_loss): 0.012936\n",
            "반복(Epoch): 87, Pre-Training 손실 함수(pretraining_loss): 0.013517\n",
            "반복(Epoch): 88, Pre-Training 손실 함수(pretraining_loss): 0.012737\n",
            "반복(Epoch): 89, Pre-Training 손실 함수(pretraining_loss): 0.010391\n",
            "반복(Epoch): 90, Pre-Training 손실 함수(pretraining_loss): 0.013707\n",
            "반복(Epoch): 91, Pre-Training 손실 함수(pretraining_loss): 0.012065\n",
            "반복(Epoch): 92, Pre-Training 손실 함수(pretraining_loss): 0.011748\n",
            "반복(Epoch): 93, Pre-Training 손실 함수(pretraining_loss): 0.013348\n",
            "반복(Epoch): 94, Pre-Training 손실 함수(pretraining_loss): 0.011532\n",
            "반복(Epoch): 95, Pre-Training 손실 함수(pretraining_loss): 0.010424\n",
            "반복(Epoch): 96, Pre-Training 손실 함수(pretraining_loss): 0.011706\n",
            "반복(Epoch): 97, Pre-Training 손실 함수(pretraining_loss): 0.010879\n",
            "반복(Epoch): 98, Pre-Training 손실 함수(pretraining_loss): 0.012187\n",
            "반복(Epoch): 99, Pre-Training 손실 함수(pretraining_loss): 0.011329\n",
            "반복(Epoch): 100, Pre-Training 손실 함수(pretraining_loss): 0.010895\n",
            "Step 1 : MNIST 데이터 재구축을 위한 오토인코더 최적화 완료(Pre-Training)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs + 100):\n",
        "  for batch_x, batch_y in train_data:\n",
        "    batch_y = tf.one_hot(batch_y, depth=10)\n",
        "    _, finetuning_loss_print = finetuning_train_step(AutoEncoder_model, SoftmaxClassifier_model, batch_x, batch_y), finetuning_cross_entropy_loss(SoftmaxClassifier_model(AutoEncoder_model(batch_x)[1]), batch_y)\n",
        "  if epoch % display_step == 0:\n",
        "    print(\"반복(Epoch): %d, Fine-tuning 손실 함수(finetuning_loss): %f\" % ((epoch + 1), finetuning_loss_print))\n",
        "print(\"Step 2 : MNIST 데이터 분류를 위한 오토인코더+Softmax 분류기 최적화 완료(Fine-Tuning)\")"
      ],
      "metadata": {
        "id": "fdnzHpf5P857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77184553-c025-43bc-8c27-cf55a3f5e976"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "반복(Epoch): 129, Fine-tuning 손실 함수(finetuning_loss): 0.021641\n",
            "반복(Epoch): 130, Fine-tuning 손실 함수(finetuning_loss): 0.075768\n",
            "반복(Epoch): 131, Fine-tuning 손실 함수(finetuning_loss): 0.017730\n",
            "반복(Epoch): 132, Fine-tuning 손실 함수(finetuning_loss): 0.022574\n",
            "반복(Epoch): 133, Fine-tuning 손실 함수(finetuning_loss): 0.011989\n",
            "반복(Epoch): 134, Fine-tuning 손실 함수(finetuning_loss): 0.020820\n",
            "반복(Epoch): 135, Fine-tuning 손실 함수(finetuning_loss): 0.025474\n",
            "반복(Epoch): 136, Fine-tuning 손실 함수(finetuning_loss): 0.056525\n",
            "반복(Epoch): 137, Fine-tuning 손실 함수(finetuning_loss): 0.022043\n",
            "반복(Epoch): 138, Fine-tuning 손실 함수(finetuning_loss): 0.006959\n",
            "반복(Epoch): 139, Fine-tuning 손실 함수(finetuning_loss): 0.011196\n",
            "반복(Epoch): 140, Fine-tuning 손실 함수(finetuning_loss): 0.018533\n",
            "반복(Epoch): 141, Fine-tuning 손실 함수(finetuning_loss): 0.032099\n",
            "반복(Epoch): 142, Fine-tuning 손실 함수(finetuning_loss): 0.022752\n",
            "반복(Epoch): 143, Fine-tuning 손실 함수(finetuning_loss): 0.039507\n",
            "반복(Epoch): 144, Fine-tuning 손실 함수(finetuning_loss): 0.016629\n",
            "반복(Epoch): 145, Fine-tuning 손실 함수(finetuning_loss): 0.011497\n",
            "반복(Epoch): 146, Fine-tuning 손실 함수(finetuning_loss): 0.028711\n",
            "반복(Epoch): 147, Fine-tuning 손실 함수(finetuning_loss): 0.040292\n",
            "반복(Epoch): 148, Fine-tuning 손실 함수(finetuning_loss): 0.011548\n",
            "반복(Epoch): 149, Fine-tuning 손실 함수(finetuning_loss): 0.017699\n",
            "반복(Epoch): 150, Fine-tuning 손실 함수(finetuning_loss): 0.020461\n",
            "반복(Epoch): 151, Fine-tuning 손실 함수(finetuning_loss): 0.029421\n",
            "반복(Epoch): 152, Fine-tuning 손실 함수(finetuning_loss): 0.013674\n",
            "반복(Epoch): 153, Fine-tuning 손실 함수(finetuning_loss): 0.011641\n",
            "반복(Epoch): 154, Fine-tuning 손실 함수(finetuning_loss): 0.013312\n",
            "반복(Epoch): 155, Fine-tuning 손실 함수(finetuning_loss): 0.012145\n",
            "반복(Epoch): 156, Fine-tuning 손실 함수(finetuning_loss): 0.051290\n",
            "반복(Epoch): 157, Fine-tuning 손실 함수(finetuning_loss): 0.020327\n",
            "반복(Epoch): 158, Fine-tuning 손실 함수(finetuning_loss): 0.018436\n",
            "반복(Epoch): 159, Fine-tuning 손실 함수(finetuning_loss): 0.008713\n",
            "반복(Epoch): 160, Fine-tuning 손실 함수(finetuning_loss): 0.017217\n",
            "반복(Epoch): 161, Fine-tuning 손실 함수(finetuning_loss): 0.016129\n",
            "반복(Epoch): 162, Fine-tuning 손실 함수(finetuning_loss): 0.013489\n",
            "반복(Epoch): 163, Fine-tuning 손실 함수(finetuning_loss): 0.020391\n",
            "반복(Epoch): 164, Fine-tuning 손실 함수(finetuning_loss): 0.011980\n",
            "반복(Epoch): 165, Fine-tuning 손실 함수(finetuning_loss): 0.017293\n",
            "반복(Epoch): 166, Fine-tuning 손실 함수(finetuning_loss): 0.033852\n",
            "반복(Epoch): 167, Fine-tuning 손실 함수(finetuning_loss): 0.017545\n",
            "반복(Epoch): 168, Fine-tuning 손실 함수(finetuning_loss): 0.005364\n",
            "반복(Epoch): 169, Fine-tuning 손실 함수(finetuning_loss): 0.018010\n",
            "반복(Epoch): 170, Fine-tuning 손실 함수(finetuning_loss): 0.015939\n",
            "반복(Epoch): 171, Fine-tuning 손실 함수(finetuning_loss): 0.013983\n",
            "반복(Epoch): 172, Fine-tuning 손실 함수(finetuning_loss): 0.014656\n",
            "반복(Epoch): 173, Fine-tuning 손실 함수(finetuning_loss): 0.010302\n",
            "반복(Epoch): 174, Fine-tuning 손실 함수(finetuning_loss): 0.048795\n",
            "반복(Epoch): 175, Fine-tuning 손실 함수(finetuning_loss): 0.018940\n",
            "반복(Epoch): 176, Fine-tuning 손실 함수(finetuning_loss): 0.022258\n",
            "반복(Epoch): 177, Fine-tuning 손실 함수(finetuning_loss): 0.011181\n",
            "반복(Epoch): 178, Fine-tuning 손실 함수(finetuning_loss): 0.016061\n",
            "반복(Epoch): 179, Fine-tuning 손실 함수(finetuning_loss): 0.006795\n",
            "반복(Epoch): 180, Fine-tuning 손실 함수(finetuning_loss): 0.014264\n",
            "반복(Epoch): 181, Fine-tuning 손실 함수(finetuning_loss): 0.021155\n",
            "반복(Epoch): 182, Fine-tuning 손실 함수(finetuning_loss): 0.080280\n",
            "반복(Epoch): 183, Fine-tuning 손실 함수(finetuning_loss): 0.025303\n",
            "반복(Epoch): 184, Fine-tuning 손실 함수(finetuning_loss): 0.018540\n",
            "반복(Epoch): 185, Fine-tuning 손실 함수(finetuning_loss): 0.010576\n",
            "반복(Epoch): 186, Fine-tuning 손실 함수(finetuning_loss): 0.009656\n",
            "반복(Epoch): 187, Fine-tuning 손실 함수(finetuning_loss): 0.013203\n",
            "반복(Epoch): 188, Fine-tuning 손실 함수(finetuning_loss): 0.018013\n",
            "반복(Epoch): 189, Fine-tuning 손실 함수(finetuning_loss): 0.007467\n",
            "반복(Epoch): 190, Fine-tuning 손실 함수(finetuning_loss): 0.006442\n",
            "반복(Epoch): 191, Fine-tuning 손실 함수(finetuning_loss): 0.014267\n",
            "반복(Epoch): 192, Fine-tuning 손실 함수(finetuning_loss): 0.009187\n",
            "반복(Epoch): 193, Fine-tuning 손실 함수(finetuning_loss): 0.009849\n",
            "반복(Epoch): 194, Fine-tuning 손실 함수(finetuning_loss): 0.011671\n",
            "반복(Epoch): 195, Fine-tuning 손실 함수(finetuning_loss): 0.006200\n",
            "반복(Epoch): 196, Fine-tuning 손실 함수(finetuning_loss): 0.021124\n",
            "반복(Epoch): 197, Fine-tuning 손실 함수(finetuning_loss): 0.030862\n",
            "반복(Epoch): 198, Fine-tuning 손실 함수(finetuning_loss): 0.005850\n",
            "반복(Epoch): 199, Fine-tuning 손실 함수(finetuning_loss): 0.007391\n",
            "반복(Epoch): 200, Fine-tuning 손실 함수(finetuning_loss): 0.008083\n",
            "Step 2 : MNIST 데이터 분류를 위한 오토인코더+Softmax 분류기 최적화 완료(Fine-Tuning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"정확도(오토인코더+Softmax 분류기): %f\" % compute_accuracy(SoftmaxClassifier_model(AutoEncoder_model(x_test)[1]), tf.one_hot(y_test, depth=10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f9EfVsaax3g",
        "outputId": "8857f5e9-0ac5-4443-c26c-23fc693c1bfa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정확도(오토인코더+Softmax 분류기): 0.961200\n"
          ]
        }
      ]
    }
  ]
}